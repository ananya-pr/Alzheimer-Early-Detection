{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Anomaly Detection with Variational Autoencoders and Encoded Cluster Plots"
      ],
      "metadata": {
        "id": "lKsyhMZ9Dk7p"
      },
      "id": "lKsyhMZ9Dk7p"
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is my work for constructing and testing a Variational Autoencoder (VAE) that helps detect Alzheimer's Disease early using images from three linked conditions."
      ],
      "metadata": {
        "id": "BuhLxZR-DkNm"
      },
      "id": "BuhLxZR-DkNm"
    },
    {
      "cell_type": "markdown",
      "source": [
        "First, I did all relevant imports for this project and seeded everything."
      ],
      "metadata": {
        "id": "nZVvjH8_D0q-"
      },
      "id": "nZVvjH8_D0q-"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d51de17a-eeb6-47e0-b638-6c610ea9fe5c",
      "metadata": {
        "id": "d51de17a-eeb6-47e0-b638-6c610ea9fe5c"
      },
      "outputs": [],
      "source": [
        "## All Imports ##\n",
        "import os\n",
        "import random\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from tqdm.notebook import tqdm\n",
        "from sklearn.manifold import TSNE\n",
        "import pandas as pd\n",
        "from PIL import Image\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, Subset\n",
        "from torchvision.datasets import ImageFolder\n",
        "from torchvision import transforms\n",
        "\n",
        "def set_seed(seed):\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    random.seed(seed)\n",
        "\n",
        "set_seed(0)\n",
        "\n",
        "train_set_pct = 1.0"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, I mounted my google drive, where I unzipped all files downloaded in my local drive. All data was obtained from the article \"Identifying Medical Diagnoses and Treatable Diseases by Image-Based Deep Learning\" published in the journal Cell.\n",
        "\n",
        "Citation for Data Usage:\n",
        "Kermany D, Goldbaum M, Cai W et al. Identifying Medical Diagnoses and Treatable Diseases by Image-Based Deep Learning. Cell. 2018; 172(5):1122-1131. doi:10.1016/j.cell.2018.02.010."
      ],
      "metadata": {
        "id": "qH2jkRXoD5b6"
      },
      "id": "qH2jkRXoD5b6"
    },
    {
      "cell_type": "code",
      "source": [
        "#MOUNTING THE DRIVE\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "!rsync -ah -progress \"/content/drive/MyDrive/Colab Notebooks/Kermany Data/ZhangLabData.zip\" \"/content\"\n",
        "!unzip /content/ZhangLabData.zip\n",
        "\n",
        "PATH_TRAIN = \"/content/CellData/OCT/train\"\n",
        "PATH_TEST = \"/content/CellData/OCT/test\""
      ],
      "metadata": {
        "id": "gL-iDgm1R_kZ",
        "collapsed": true
      },
      "id": "gL-iDgm1R_kZ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "c73516dc-c21f-4345-b4f7-cc2d6f487df9",
      "metadata": {
        "id": "c73516dc-c21f-4345-b4f7-cc2d6f487df9"
      },
      "source": [
        "## Preparing the Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "I defined the transformations, a class for loading the images which is used later, and built the train and test set for the optical coherence tomography (OCT) images. I also split off a random subset of the train data to train my EncoderForClassifier which is defined later."
      ],
      "metadata": {
        "id": "kHZI1TgxEZdV"
      },
      "id": "kHZI1TgxEZdV"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "132bda24-6f5e-4007-9bce-73922a857fa4",
      "metadata": {
        "id": "132bda24-6f5e-4007-9bce-73922a857fa4",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "GRAY_MEAN = [0.5]\n",
        "GRAY_STD = [0.5]\n",
        "\n",
        "train_transform = transforms.Compose(\n",
        "    [\n",
        "        transforms.Lambda(lambda x: x.convert(\"L\")),\n",
        "        transforms.Resize((128,128)),\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=GRAY_MEAN, std=GRAY_STD)\n",
        "    ]\n",
        ")\n",
        "\n",
        "val_transform = transforms.Compose(\n",
        "    [\n",
        "        transforms.Lambda(lambda x: x.convert(\"L\")),\n",
        "        transforms.Resize((128,128)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=GRAY_MEAN, std=GRAY_STD)\n",
        "    ]\n",
        ")\n",
        "\n",
        "\"\"\"\n",
        "Class for Loading Images\n",
        "\"\"\"\n",
        "class ImageLoaderDataset(nn.Module):\n",
        "    def __init__(self, path_to_folder, transform):\n",
        "        \"\"\"\n",
        "        Initializes instance of the ImageLoaderDataset Class\n",
        "\n",
        "        Parameters:\n",
        "        self (ImageLoaderDataset): instance of the class\n",
        "        path_to_folder (str): the path to the data\n",
        "        transform (transforms): the transformation to the data\n",
        "        \"\"\"\n",
        "\n",
        "        self.path_to_folder = path_to_folder\n",
        "        self.training_files = os.listdir(path_to_folder)\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        \"\"\"\n",
        "        Returns the length of the data files\n",
        "\n",
        "        Parameter:\n",
        "        self (ImageLoaderDataset): instance of the class\n",
        "\n",
        "        Returns:\n",
        "        int: the length of training files\n",
        "        \"\"\"\n",
        "        return len(self.training_files)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        \"\"\"\n",
        "        Gets the image at an index in the training files\n",
        "\n",
        "        Parameters:\n",
        "        self (ImageLoaderDataset): instance of the class\n",
        "        idx (int): the index of the image you are returning\n",
        "\n",
        "        Returns:\n",
        "        Image: the image at that index\n",
        "        \"\"\"\n",
        "        sample = self.training_files[idx]\n",
        "        path_to_sample = os.path.join(self.path_to_folder, sample)\n",
        "\n",
        "        image = Image.open(path_to_sample).convert(\"L\")\n",
        "        image = self.transform(image)\n",
        "\n",
        "        return image\n",
        "\n",
        "## Using Image Folder to Build the Train and Test Dataset ##\n",
        "train_set = ImageFolder(PATH_TRAIN, transform=train_transform)\n",
        "test_set = ImageFolder(PATH_TEST, transform=train_transform)\n",
        "\n",
        "## Assigning Each Class an index ##\n",
        "data_classes = {idx: val for (idx, val) in enumerate(train_set.classes)}\n",
        "\n",
        "## Choosing a Random Subset for Pretraining ##\n",
        "random_indices = random.sample(range(len(train_set)), int(len(train_set) * train_set_pct))\n",
        "train_set = Subset(train_set, random_indices)\n",
        "\n",
        "print(f\"Training with {len(train_set)} samples\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "62e7519b-7b02-4f08-ac63-eece6489f023",
      "metadata": {
        "id": "62e7519b-7b02-4f08-ac63-eece6489f023"
      },
      "source": [
        "## Defining ResidualBlocks & The Encoder"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, I defined a class for ResidualBlocks and the other key encoding classes: the EncoderBlocks, the Encoder, and the EncoderForClassifier, which I will later train on the subset of the data."
      ],
      "metadata": {
        "id": "JNQfB_kHEzbr"
      },
      "id": "JNQfB_kHEzbr"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8e879f6b-72bf-49d6-b1e0-ded9a424bd75",
      "metadata": {
        "id": "8e879f6b-72bf-49d6-b1e0-ded9a424bd75"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "ResidualBlock: a class that represents a block of residual connections\n",
        "\"\"\"\n",
        "class ResidualBlock(nn.Module):\n",
        "\n",
        "    def __init__(self,\n",
        "                 in_channels,\n",
        "                 out_channels,\n",
        "                 dropout_p=0.0):\n",
        "        \"\"\"\n",
        "        Initializes the ResidualBlock class.\n",
        "\n",
        "        Parameters:\n",
        "        self (Residual Block): instance of the class\n",
        "        in_channels (int): number of input channels\n",
        "        out_channels (int): number of output channels\n",
        "        dropout_p (float): dropout probability percent if you are randomly turning off some % of nodes, set to 0\n",
        "        \"\"\"\n",
        "\n",
        "        super().__init__()\n",
        "\n",
        "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=1, padding=\"same\")\n",
        "        self.norm1 = nn.BatchNorm2d(out_channels)\n",
        "\n",
        "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=\"same\")\n",
        "        self.norm2 = nn.BatchNorm2d(out_channels)\n",
        "\n",
        "        self.identity_conv = nn.Identity()\n",
        "        if in_channels != out_channels:\n",
        "            self.identity_conv = nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Forward function for ResidualBlock. Applies a residual connection by applying a convolution\n",
        "        that goes from the input channels to the output channels with stride=1, applies a BatchNorm2d and\n",
        "        then the activation function. Applies a second convolution from output channels to output channels and\n",
        "        a BatchNorm2d and then activation again. Finally, it has the residual effect by adding the identity on the residual.\n",
        "\n",
        "        Parameters:\n",
        "        self (ResidualBlock): instance of the class\n",
        "        x: the input to the block\n",
        "\n",
        "        Returns:\n",
        "        x: the output after adding the residual\n",
        "        \"\"\"\n",
        "        residual = x\n",
        "\n",
        "        x = self.conv1(x)\n",
        "        x = self.norm1(x)\n",
        "        x = F.relu(x)\n",
        "\n",
        "        x = self.conv2(x)\n",
        "        x = self.norm2(x)\n",
        "        x = F.relu(x)\n",
        "\n",
        "        x = x + self.identity_conv(residual)\n",
        "\n",
        "        return x\n",
        "\n",
        "\"\"\"\n",
        "EncoderBlock: a class that defines the structure and ResidualBlocks that make up the Encoder\n",
        "of my VAE.\n",
        "\"\"\"\n",
        "class EncoderBlock(nn.Module):\n",
        "    def __init__(self,\n",
        "                 in_channels,\n",
        "                 out_channels,\n",
        "                 residual_blocks_per_group=1,\n",
        "                 downsample=True,\n",
        "                 dropout_p=0.0):\n",
        "      \"\"\"\n",
        "      Initializes an instance of the EncoderBlock class.\n",
        "\n",
        "      Parameters:\n",
        "      self (EncoderBlock): instance of the class\n",
        "      in_channels (int): number of input channels\n",
        "      out_channels (int): number of output channels\n",
        "      residual_blocks_per_group (int): number of ResidualBlocks per group, set to 1\n",
        "      downsample (bool): True if downsampling, else False\n",
        "      dropout_p(float): % of nodes randomly turned off, set to 0.0\n",
        "      \"\"\"\n",
        "\n",
        "      super().__init__()\n",
        "\n",
        "      self.encoder_block = nn.ModuleList([])\n",
        "      for i in range(residual_blocks_per_group):\n",
        "          in_c = in_channels if i == 0 else out_channels\n",
        "          self.encoder_block.append(\n",
        "              ResidualBlock(in_c, out_channels, dropout_p)\n",
        "          )\n",
        "\n",
        "      if downsample:\n",
        "          self.encoder_block.append(\n",
        "              nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=2, padding=1)\n",
        "          )\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Forward function for the EncoderBlock that specifies how input flows through the network.\n",
        "\n",
        "        Parameters:\n",
        "        self (EncoderBlock): instance of the class\n",
        "        x: the input passed to the layer\n",
        "        \"\"\"\n",
        "\n",
        "        for block in self.encoder_block:\n",
        "            x = block(x)\n",
        "\n",
        "        return x\n",
        "\"\"\"\n",
        "Encoder: encoder class for the VAE, which has a sequence of EncoderBlocks which go through a channel pattern\n",
        "and which downsample.\n",
        "\"\"\"\n",
        "class Encoder(nn.Module):\n",
        "    \"\"\"\n",
        "    Initializes the Encoder class\n",
        "\n",
        "    Parameters:\n",
        "    self (Encoder): instance of the class\n",
        "    in_channels (int): number of input channels\n",
        "    channel_pattern (array): the pattern of number of channels for the encoder.\n",
        "                            If None, default is [32,64,128,256]\n",
        "    residual_blocks_per_group (int): the number of residual blocks per group, set to 1\n",
        "    dropout_p (float): percent of nodes randomly turned off, set to 0.0\n",
        "    \"\"\"\n",
        "    def __init__(self,\n",
        "                  in_channels,\n",
        "                  channel_pattern=None,\n",
        "                  residual_blocks_per_group=1,\n",
        "                  dropout_p=0.0):\n",
        "\n",
        "        super().__init__()\n",
        "\n",
        "        self.channel_pattern = channel_pattern\n",
        "        if channel_pattern is None:\n",
        "            self.channel_pattern = [32,64,128,256]\n",
        "\n",
        "\n",
        "        self.blocks1 = EncoderBlock(in_channels,\n",
        "                                    self.channel_pattern[0],\n",
        "                                    residual_blocks_per_group=residual_blocks_per_group,\n",
        "                                    downsample=True,\n",
        "                                    dropout_p=dropout_p)\n",
        "\n",
        "        self.blocks2 = EncoderBlock(self.channel_pattern[0],\n",
        "                                    self.channel_pattern[1],\n",
        "                                    residual_blocks_per_group=residual_blocks_per_group,\n",
        "                                    downsample=True,\n",
        "                                    dropout_p=dropout_p)\n",
        "\n",
        "        self.blocks3 = EncoderBlock(self.channel_pattern[1],\n",
        "                                    self.channel_pattern[2],\n",
        "                                    residual_blocks_per_group=residual_blocks_per_group,\n",
        "                                    downsample=True,\n",
        "                                    dropout_p=dropout_p)\n",
        "\n",
        "        self.blocks4 = EncoderBlock(self.channel_pattern[2],\n",
        "                                    self.channel_pattern[3],\n",
        "                                    residual_blocks_per_group=residual_blocks_per_group,\n",
        "                                    downsample=True,\n",
        "                                    dropout_p=dropout_p)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        The forward function for the Encoder that specifies how input flows through the\n",
        "        network. It goes through all 4 encoder blocks.\n",
        "        \"\"\"\n",
        "\n",
        "        x = self.blocks1(x)\n",
        "        x = self.blocks2(x)\n",
        "        x = self.blocks3(x)\n",
        "        x = self.blocks4(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "\"\"\"\n",
        "EncoderForClassifier: This is the encoder which learns the classification, being\n",
        "trained later on just a percent of the data. I will use this to load pretrained weights\n",
        "to the encoder of the VAE later.\n",
        "\"\"\"\n",
        "class EncoderForClassifier(nn.Module):\n",
        "    \"\"\"\n",
        "    Initializes the EncoderForClassifier.\n",
        "\n",
        "    Parameters:\n",
        "    self (Encoder): instance of the class\n",
        "    num_classes (int): number of classes, set to four since the OCT dataset has four\n",
        "    in_channels (int): number of input channels\n",
        "    channel_pattern (array): the pattern of number of channels for the encoder.\n",
        "                            If None, default is [32,64,128,256]\n",
        "    residual_blocks_per_group (int): the number of residual blocks per group, set to 1\n",
        "    dropout_p (float): percent of nodes randomly turned off, set to 0.0\n",
        "    \"\"\"\n",
        "    def __init__(self,\n",
        "                  num_classes=4,\n",
        "                  in_channels=3,\n",
        "                  channel_pattern=None,\n",
        "                  residual_blocks_per_group=1,\n",
        "                  dropout_p=0.0):\n",
        "        super().__init__()\n",
        "\n",
        "        self.encoder = Encoder(in_channels,\n",
        "                                channel_pattern=None,\n",
        "                                residual_blocks_per_group=1,\n",
        "                                dropout_p=0.0)\n",
        "\n",
        "        self.head = nn.Linear(self.encoder.channel_pattern[-1] * 8 * 8, num_classes)\n",
        "    \"\"\"\n",
        "    Forward function for the EncoderForClassifier\n",
        "\n",
        "    Parameters:\n",
        "    self (EncoderForClassifier): instance of the class\n",
        "    x: input passed to EncoderForClassifier\n",
        "    \"\"\"\n",
        "    def forward(self, x):\n",
        "        x = self.encoder(x)\n",
        "        x = x.flatten(1)\n",
        "        x = self.head(x)\n",
        "        return x\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "55ae5582-1b13-4560-84ed-f6d1dfbed6eb",
      "metadata": {
        "id": "55ae5582-1b13-4560-84ed-f6d1dfbed6eb"
      },
      "source": [
        "## Train Classifier on Subset"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here, I wrote my training script and trained the EncoderForClassifier on a subset of the data. I also saved the state dict of the encoder with the file name \"vae18_encoder\" + str(train_set_pct*100) +\"_c.pt,\" so for example, if I trained the model on 10% it would be saved to vae18_encoder10.0_c.pt. I saved all these state dicts through my experimentation."
      ],
      "metadata": {
        "id": "BMk_WIvWFPxR"
      },
      "id": "BMk_WIvWFPxR"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "90ac2920-417b-45cd-85e1-08aedd356f84",
      "metadata": {
        "id": "90ac2920-417b-45cd-85e1-08aedd356f84",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Training function for the EncoderForClassifier\n",
        "\n",
        "Parameters:\n",
        "model: the model being trained\n",
        "device: the device being used\n",
        "epochs (int): number of epochs\n",
        "optimizer: the optimizer\n",
        "loss_fn: the loss function\n",
        "trainloader: the data loader for the training data\n",
        "valloader: the data loader for the test data\n",
        "\n",
        "Returns:\n",
        "log_training: the training log that specifies the epoch, training loss, training\n",
        "accuracy, validation loss, and validation accuracy.\n",
        "model: the model\n",
        "\"\"\"\n",
        "def train(model, device, epochs, optimizer, loss_fn, trainloader, valloader):\n",
        "    log_training = {\"epoch\": [],\n",
        "                    \"training_loss\": [],\n",
        "                    \"training_acc\": [],\n",
        "                    \"validation_loss\": [],\n",
        "                    \"validation_acc\": []}\n",
        "\n",
        "    model = model.to(device)\n",
        "\n",
        "    for epoch in range(1, epochs + 1):\n",
        "        print(f\"Starting Epoch {epoch}\")\n",
        "        training_losses, training_accuracies = [], []\n",
        "        validation_losses, validation_accuracies = [], []\n",
        "\n",
        "        model.train()\n",
        "        for image, label in tqdm(trainloader):\n",
        "            image, label = image.to(device), label.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            out = model.forward(image)\n",
        "\n",
        "            ## Calculate Loss ##\n",
        "            loss = loss_fn(out, label)\n",
        "            training_losses.append(loss.item())\n",
        "\n",
        "            ## Calculate Accuracy ##\n",
        "            predictions = torch.argmax(out, axis=1)\n",
        "            accuracy = (predictions == label).sum() / len(predictions)\n",
        "            training_accuracies.append(accuracy.item())\n",
        "\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        model.eval() ## BatchNorm is Off ##\n",
        "        for image, label in tqdm(valloader):\n",
        "            image, label = image.to(device), label.to(device)\n",
        "            with torch.no_grad():\n",
        "                out = model.forward(image)\n",
        "\n",
        "                ## Calculate Loss ##\n",
        "                loss = loss_fn(out, label)\n",
        "                validation_losses.append(loss.item())\n",
        "\n",
        "                ## Calculate Accuracy ##\n",
        "                predictions = torch.argmax(out, axis=1)\n",
        "                accuracy = (predictions == label).sum() / len(predictions)\n",
        "                validation_accuracies.append(accuracy.item())\n",
        "\n",
        "        training_loss_mean, training_acc_mean = np.mean(training_losses), np.mean(training_accuracies)\n",
        "        valid_loss_mean, valid_acc_mean = np.mean(validation_losses), np.mean(validation_accuracies)\n",
        "\n",
        "        log_training[\"epoch\"].append(epoch)\n",
        "        log_training[\"training_loss\"].append(training_loss_mean)\n",
        "        log_training[\"training_acc\"].append(training_acc_mean)\n",
        "        log_training[\"validation_loss\"].append(valid_loss_mean)\n",
        "        log_training[\"validation_acc\"].append(valid_acc_mean)\n",
        "\n",
        "        print(\"Training Loss:\", training_loss_mean)\n",
        "        print(\"Training Acc:\", training_acc_mean)\n",
        "        print(\"Validation Loss:\", valid_loss_mean)\n",
        "        print(\"Validation Acc:\", valid_acc_mean)\n",
        "\n",
        "    return log_training, model\n",
        "\n",
        "model = EncoderForClassifier(in_channels=1)\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "epochs = 5\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=0.0001, weight_decay=0.01)\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "trainloader = DataLoader(train_set, batch_size=64, shuffle=True, num_workers=16)\n",
        "testloader = DataLoader(test_set, batch_size=64, shuffle=False, num_workers=16)\n",
        "\n",
        "log_training, model = train(model=model,\n",
        "                            device=device,\n",
        "                            epochs=epochs,\n",
        "                            optimizer=optimizer,\n",
        "                            loss_fn=loss_fn,\n",
        "                            trainloader=trainloader,\n",
        "                            valloader=testloader)\n",
        "\n",
        "## Saving the State Dict ##\n",
        "torch.save(model.encoder.state_dict(), \"vae18_encoder\" + str(train_set_pct*100) +\"_c.pt\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "514e7746-2d0b-4780-848b-bb5edd992058",
      "metadata": {
        "id": "514e7746-2d0b-4780-848b-bb5edd992058"
      },
      "source": [
        "### Defining my VAE"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "I defined the main Variational Autoencoder model. Of course, I had to define relevant classes including the class for upsampling and the Decoder (along with the DecoderBlock) for this task."
      ],
      "metadata": {
        "id": "ix_euvJCFqrA"
      },
      "id": "ix_euvJCFqrA"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a0fab95b-915c-4037-9015-9c912b948d7f",
      "metadata": {
        "id": "a0fab95b-915c-4037-9015-9c912b948d7f"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "UpsampleBlock: Class for Upsampling for the VAE since Residual COnnections alone don't\n",
        "change image resolution\n",
        "\"\"\"\n",
        "class UpsampleBlock(nn.Module):\n",
        "    def __init__(self,\n",
        "                  in_channels,\n",
        "                  out_channels):\n",
        "        \"\"\"\n",
        "        Initializes UpsampleBlock\n",
        "\n",
        "        Parameters:\n",
        "        self (UpsampleBlock): instance of the class\n",
        "        in_channels (int): number of input channels\n",
        "        out_channels (int): number of output channels\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "\n",
        "        self.up = nn.Sequential(\n",
        "            nn.Upsample(scale_factor=2),\n",
        "            nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=1, padding=\"same\")\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Forward function for UpsampleBlock that applies upsampling to the input\n",
        "\n",
        "        Parameters:\n",
        "        self (UpSampleBlock): instance of the class\n",
        "        x: input passed into model\n",
        "        \"\"\"\n",
        "        return self.up(x)\n",
        "\n",
        "\"\"\"\n",
        "DecoderBlock: a class that details the structure of residual blocks in the decoder.\n",
        "\"\"\"\n",
        "class DecoderBlock(nn.Module):\n",
        "    def __init__(self,\n",
        "                 in_channels,\n",
        "                 out_channels,\n",
        "                 residual_blocks_per_group=1,\n",
        "                 upsample=True,\n",
        "                 dropout_p=0.0):\n",
        "        \"\"\"\n",
        "        Initializes the DecoderBlock.\n",
        "\n",
        "        Parameters:\n",
        "        self (DecoderBlock): instance of the class\n",
        "        in_channels (int): number of input channels\n",
        "        out_channels (int): number of output channels\n",
        "        residual_blocks_per_group (int): number of ResidualBlocks per group, set to 1\n",
        "        upsample (bool): True if upsampling, else False\n",
        "        dropout_p(float): % of nodes randomly turned off, set to 0.0\n",
        "\n",
        "        \"\"\"\n",
        "\n",
        "        super().__init__()\n",
        "\n",
        "        self.encoder_block = nn.ModuleList([])\n",
        "        for i in range(residual_blocks_per_group):\n",
        "            in_c = in_channels if i == 0 else out_channels\n",
        "            self.encoder_block.append(\n",
        "                ResidualBlock(in_c, out_channels, dropout_p)\n",
        "            )\n",
        "\n",
        "        if upsample:\n",
        "            self.encoder_block.append(\n",
        "                UpsampleBlock(out_channels, out_channels)\n",
        "            )\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Forward function for the DecoderBlock that specifies how input\n",
        "        flows through the network's layers.\n",
        "\n",
        "        Parameters:\n",
        "        self (DecoderBlock): instance of the class\n",
        "        x: the input passed to the layer\n",
        "        \"\"\"\n",
        "        for block in self.encoder_block:\n",
        "            x = block(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "\"\"\"\n",
        "Decoder: decoder class for the VAE, which has a sequence of DecoderBlocks which go through a channel pattern\n",
        "and which upsample.\n",
        "\"\"\"\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self,\n",
        "                 out_channels,\n",
        "                 channel_pattern=None,\n",
        "                 residual_blocks_per_group=1,\n",
        "                 dropout_p=0.0):\n",
        "        \"\"\"\n",
        "        Initializes the Encoder class\n",
        "\n",
        "        Parameters:\n",
        "        self (Decoder): instance of the class\n",
        "        out_channels (int): number of output channels\n",
        "        channel_pattern (array): the pattern of number of channels for the encoder.\n",
        "                                  If None, default is [32,64,128,256]\n",
        "        residual_blocks_per_group (int): the number of residual blocks per group, set to 1\n",
        "        dropout_p (float): percent of nodes randomly turned off, set to 0.0\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "\n",
        "        self.channel_pattern = channel_pattern\n",
        "        if channel_pattern is None:\n",
        "            self.channel_pattern = [32,64,128,256]\n",
        "        self.channel_pattern = list(reversed(self.channel_pattern))\n",
        "\n",
        "        self.blocks1 = DecoderBlock(self.channel_pattern[0],\n",
        "                                    self.channel_pattern[1],\n",
        "                                    residual_blocks_per_group=residual_blocks_per_group,\n",
        "                                    upsample=True,\n",
        "                                    dropout_p=dropout_p)\n",
        "\n",
        "        self.blocks2 = DecoderBlock(self.channel_pattern[1],\n",
        "                                    self.channel_pattern[2],\n",
        "                                    residual_blocks_per_group=residual_blocks_per_group,\n",
        "                                    upsample=True,\n",
        "                                    dropout_p=dropout_p)\n",
        "\n",
        "        self.blocks3 = DecoderBlock(self.channel_pattern[2],\n",
        "                                    self.channel_pattern[3],\n",
        "                                    residual_blocks_per_group=residual_blocks_per_group,\n",
        "                                    upsample=True,\n",
        "                                    dropout_p=dropout_p)\n",
        "\n",
        "        self.blocks4 = DecoderBlock(self.channel_pattern[3],\n",
        "                                    out_channels,\n",
        "                                    residual_blocks_per_group=residual_blocks_per_group,\n",
        "                                    upsample=True,\n",
        "                                    dropout_p=dropout_p)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        The forward function for the Decoder that specifies how input flows through the\n",
        "        network. It goes through all 4 decoder blocks.\n",
        "        \"\"\"\n",
        "        x = self.blocks1(x)\n",
        "        x = self.blocks2(x)\n",
        "        x = self.blocks3(x)\n",
        "        x = self.blocks4(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "\"\"\"\n",
        "VAE: Variational Autoencoder\n",
        "\"\"\"\n",
        "class VAE(nn.Module):\n",
        "    def __init__(self,\n",
        "                 in_channels,\n",
        "                 latent_channels=16,\n",
        "                 channel_pattern=None,\n",
        "                 residual_blocks_per_group=1,\n",
        "                 dropout_p=0.0):\n",
        "        \"\"\"\n",
        "        Initializes the VAE class\n",
        "\n",
        "        Parameters:\n",
        "        self (VAE): instance of the class\n",
        "        in_channels (int): number of input channels\n",
        "        latent_channels (int): number of latent channels\n",
        "        channel_pattern (array): the pattern of number of channels for the encoder.\n",
        "                                  If None, default is [32,64,128,256]\n",
        "        residual_blocks_per_group (int): the number of residual blocks per group, set to 1\n",
        "        dropout_p (float): percent of nodes randomly turned off, set to 0.0\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "\n",
        "        self.channel_pattern = channel_pattern\n",
        "        if channel_pattern is None:\n",
        "            self.channel_pattern = [32,64,128,256]\n",
        "\n",
        "        self.encoder = Encoder(in_channels=in_channels,\n",
        "                               channel_pattern=self.channel_pattern,\n",
        "                               residual_blocks_per_group=residual_blocks_per_group,\n",
        "                               dropout_p=dropout_p)\n",
        "\n",
        "        self.decoder = Decoder(out_channels=in_channels,\n",
        "                               channel_pattern=self.channel_pattern,\n",
        "                               residual_blocks_per_group=residual_blocks_per_group,\n",
        "                               dropout_p=dropout_p)\n",
        "\n",
        "        self.proj_to_latent = nn.Conv2d(self.channel_pattern[-1],\n",
        "                                        latent_channels * 2,\n",
        "                                        kernel_size=1,\n",
        "                                        padding=\"same\")\n",
        "\n",
        "        self.latent_to_proj = nn.Conv2d(latent_channels,\n",
        "                                        self.channel_pattern[-1],\n",
        "                                        kernel_size=1,\n",
        "                                        padding=\"same\")\n",
        "\n",
        "    def kl_loss(self, mean, logvar):\n",
        "        \"\"\"\n",
        "        Loss function for kL divergence, finds the loss for each pixel\n",
        "        and then averages it.\n",
        "\n",
        "        Parameters:\n",
        "        self (VAE): instance of class\n",
        "        mean (float): the mean\n",
        "        logvar (float): the log variance\n",
        "        \"\"\"\n",
        "        var = torch.exp(logvar)\n",
        "\n",
        "        kl_loss = -0.5 * torch.sum(\n",
        "            1 + logvar - mean**2 - var,\n",
        "            dim=[1,2,3]\n",
        "        )\n",
        "\n",
        "        kl_loss = kl_loss.mean()\n",
        "\n",
        "        return kl_loss\n",
        "\n",
        "    def encode(self, x):\n",
        "        \"\"\"\n",
        "        Function that describes how the VAE encodes: following the channel pattern, projecting to 2*latent space\n",
        "        and then finding the mu and logvar.\n",
        "        Pretrained weights will be added for the no_grad() portion of it.\n",
        "\n",
        "        Parameters:\n",
        "        self (VAE): instance of class\n",
        "        x: the input passed in to the encoder\n",
        "\n",
        "        Returns:\n",
        "        z: the probabilistic distribution\n",
        "        mu: the mean\n",
        "        logvar: the log variance\n",
        "        \"\"\"\n",
        "        with torch.no_grad():\n",
        "            x = self.encoder(x)\n",
        "\n",
        "        x = self.proj_to_latent(x)\n",
        "\n",
        "        mu, logvar = torch.chunk(x, chunks=2, dim=1)\n",
        "        logvar = torch.clamp(logvar, min=-30, max=20)\n",
        "        sigma = torch.exp(0.5 * logvar)\n",
        "        noise = torch.randn_like(sigma, device=sigma.device, dtype=sigma.dtype)\n",
        "        z = mu + sigma * noise\n",
        "\n",
        "        return z, mu, logvar\n",
        "\n",
        "    def decode(self, x):\n",
        "        \"\"\"\n",
        "        Function that does the decoding. Goes from the latent/bottleneck space then the\n",
        "        decoder blocks.\n",
        "\n",
        "        Parameters:\n",
        "        self (VAE): instance of class\n",
        "        x: input\n",
        "        \"\"\"\n",
        "\n",
        "        x = self.latent_to_proj(x)\n",
        "        x = self.decoder(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        The forward function for the VAE. Goes through the encoder and decoder.\n",
        "\n",
        "        Parameters:\n",
        "        self (VAE): instance of class\n",
        "        x: input\n",
        "\n",
        "        Returns:\n",
        "        enc: the encoded features\n",
        "        dec: the decoded image\n",
        "        kl_loss: the kL loss\n",
        "        \"\"\"\n",
        "\n",
        "        enc, mu, logvar = self.encode(x)\n",
        "        dec = self.decode(enc)\n",
        "\n",
        "        kl_loss = self.kl_loss(mu, logvar)\n",
        "\n",
        "        return enc, dec, kl_loss\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9b9afff4-baf8-4b35-a68e-f469fb55143d",
      "metadata": {
        "id": "9b9afff4-baf8-4b35-a68e-f469fb55143d"
      },
      "source": [
        "### Training the VAE ###"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, I trained my VAE."
      ],
      "metadata": {
        "id": "F3IrKchcF9zJ"
      },
      "id": "F3IrKchcF9zJ"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e59045c4-574f-4906-9773-8d2878b23fc4",
      "metadata": {
        "id": "e59045c4-574f-4906-9773-8d2878b23fc4"
      },
      "outputs": [],
      "source": [
        "def train(model,\n",
        "          kl_weight,\n",
        "          train_set,\n",
        "          test_set,\n",
        "          batch_size,\n",
        "          training_iterations,\n",
        "          evaluation_iterations):\n",
        "    \"\"\"\n",
        "    Training Script for the VAE.\n",
        "\n",
        "    Parameters:\n",
        "    model: the model\n",
        "    kl_weight: the weight for the kL divergence loss\n",
        "    train_set: the train set\n",
        "    test_set: the test set\n",
        "    batch_size: the batch size\n",
        "    training_iterations: the number of training iterations\n",
        "    evaluation_iterations: the number of evaluation iterations\n",
        "\n",
        "    Returns:\n",
        "    model: the model\n",
        "    train_losses: the train losses\n",
        "    evaluation_losses: the evaluation losses\n",
        "    encoded_data_per_eval: the encoded data\n",
        "    \"\"\"\n",
        "\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "    model = model.to(device)\n",
        "\n",
        "    trainloader = DataLoader(train_set, batch_size=batch_size, shuffle=True, num_workers=8)\n",
        "    testloader = DataLoader(test_set, batch_size=batch_size, shuffle=False, num_workers=8)\n",
        "\n",
        "    params_to_train = filter(lambda p: p.requires_grad, model.parameters())\n",
        "    optimizer = torch.optim.AdamW(params_to_train, lr=0.0001, weight_decay=0.001)\n",
        "\n",
        "    train_loss = []\n",
        "    evaluation_loss = []\n",
        "\n",
        "    encoded_data_per_eval = []\n",
        "    train_losses = []\n",
        "    evaluation_losses = []\n",
        "\n",
        "    pbar = tqdm(range(training_iterations))\n",
        "\n",
        "    train = True\n",
        "\n",
        "    step_counter = 0\n",
        "    while train:\n",
        "\n",
        "        for images in trainloader:\n",
        "\n",
        "            images = images.to(device)\n",
        "\n",
        "            enc, dec, kl_loss = model(images)\n",
        "\n",
        "            reconstruction_loss = torch.sum((dec - images)**2) / len(images)\n",
        "\n",
        "            loss = reconstruction_loss + kl_weight * kl_loss\n",
        "\n",
        "            train_loss.append(loss.item())\n",
        "\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            if step_counter % evaluation_iterations == 0:\n",
        "\n",
        "                model.eval()\n",
        "\n",
        "                encoded_evaluations = []\n",
        "\n",
        "                for images, labels in testloader:\n",
        "\n",
        "                    images = images.to(device)\n",
        "\n",
        "                    enc, dec, kl_loss = model(images)\n",
        "                    reconstruction_loss = torch.sum((dec - images)**2) / len(images)\n",
        "\n",
        "                    loss = reconstruction_loss + kl_weight * kl_loss\n",
        "\n",
        "                    evaluation_loss.append(loss.item())\n",
        "\n",
        "                    encoded, labels = enc.cpu().flatten(1), labels.reshape(-1,1)\n",
        "\n",
        "                    encoded_evaluations.append(torch.cat((encoded, labels), axis=-1))\n",
        "\n",
        "\n",
        "                encoded_data_per_eval.append(torch.concatenate(encoded_evaluations).detach())\n",
        "\n",
        "                train_loss = np.mean(train_loss)\n",
        "                evaluation_loss = np.mean(evaluation_loss)\n",
        "\n",
        "                train_losses.append(train_loss)\n",
        "                evaluation_losses.append(evaluation_loss)\n",
        "\n",
        "                print(\"Training Loss\", train_loss)\n",
        "                print(\"Testing Loss\", evaluation_loss)\n",
        "\n",
        "                train_loss = []\n",
        "                evaluation_loss = []\n",
        "\n",
        "                model.train()\n",
        "\n",
        "            step_counter += 1\n",
        "            pbar.update(1)\n",
        "\n",
        "\n",
        "            if step_counter >= training_iterations:\n",
        "                print(\"Completed Training!\")\n",
        "                train = False\n",
        "                break\n",
        "\n",
        "    encoded_data_per_eval = [np.array(i) for i in encoded_data_per_eval]\n",
        "\n",
        "    print(\"Final Training Loss\", train_losses[-1])\n",
        "    print(\"Final Evaluation Loss\", evaluation_losses[-1])\n",
        "\n",
        "    return model, train_losses, evaluation_losses, encoded_data_per_eval\n",
        "\n",
        "model = VAE(in_channels=1)\n",
        "model.encoder.load_state_dict(torch.load(\"vae18_encoder\"+str(train_set_pct*100)+\"_c.pt\", weights_only=True))\n",
        "\n",
        "def disable_update(module):\n",
        "    for param in module.parameters():\n",
        "        param.requires_grad_(False)\n",
        "\n",
        "kl_weight = 0.5\n",
        "train_set = ImageLoaderDataset(\"/content/CellData/OCT/train/NORMAL\", transform=train_transform)\n",
        "test_set = ImageFolder(\"/content/CellData/OCT/test\", transform=val_transform)\n",
        "batch_size = 64\n",
        "training_iterations = 5000\n",
        "evaluation_iterations = 500\n",
        "\n",
        "model, train_losses, evaluation_losses, encoded_data_per_eval = train(model,\n",
        "                                                                      kl_weight,\n",
        "                                                                      train_set,\n",
        "                                                                      test_set,\n",
        "                                                                      batch_size,\n",
        "                                                                      training_iterations,\n",
        "                                                                      evaluation_iterations)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "I saved the state_dict each time.\n",
        "\n"
      ],
      "metadata": {
        "id": "8E9751mtGE18"
      },
      "id": "8E9751mtGE18"
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(model.state_dict(), \"vae18_\"+ str(train_set_pct*100)+\"_c.pt\")"
      ],
      "metadata": {
        "id": "u-6g7i7q36W9"
      },
      "id": "u-6g7i7q36W9",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "17cb4bb3-c6f2-404f-8053-19f9f75a3d4e",
      "metadata": {
        "id": "17cb4bb3-c6f2-404f-8053-19f9f75a3d4e"
      },
      "source": [
        "### Plotting the Encoder Embeddings"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "To visualize the probabilistic latent space, I plotted the encoder embeddings using TSNE and seaborn, dimension-compression and data visualization tools.\n",
        "\n",
        "I produced a scatterplot which is colored by retinal disease condition. I also saved that figure to my computer."
      ],
      "metadata": {
        "id": "m_vCy8rxGZ9p"
      },
      "id": "m_vCy8rxGZ9p"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "45350353-9ef4-4373-8601-448e73d5d996",
      "metadata": {
        "id": "45350353-9ef4-4373-8601-448e73d5d996"
      },
      "outputs": [],
      "source": [
        "encoded = encoded_data_per_eval[-1]\n",
        "encoded_images, all_labels = encoded[:, :-1], encoded[:, -1]\n",
        "\n",
        "tsne = TSNE(n_components=2)\n",
        "tsne_out = tsne.fit_transform(encoded_images)\n",
        "df = pd.DataFrame(tsne_out, columns=[\"Feature1\", \"Feature2\"])\n",
        "df[\"Label\"] = all_labels\n",
        "df[\"Label\"] = df[\"Label\"].map(data_classes)\n",
        "\n",
        "sns.scatterplot(data=df, x=\"Feature1\", y=\"Feature2\", hue=\"Label\", palette=\"Set1\")\n",
        "\n",
        "plt.title(\"Latent Space of Variational Autoencoder with encoder trained on \" + str(int(train_set_pct*100)) +\"% of the data\")\n",
        "plt.savefig(\"vae18_\" + str(train_set_pct*100) +\"clusterplot.jpeg\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Checking the Quality of Clusters"
      ],
      "metadata": {
        "id": "uSM8pWp-IK2T"
      },
      "id": "uSM8pWp-IK2T"
    },
    {
      "cell_type": "markdown",
      "source": [
        "First, in addition to my visual check of the clusters, I wanted to check the quality of clustering using the silhouette score.\n",
        "\n",
        "I calculated the silhouette score (SS) for each comparison of normal and each of the three conditions (DME, DRUSEN, and CNV).\n",
        "\n",
        "To explain briefly what the silhouette score is, it is a number ranging between -1 and 1 where more negative values indicate worse clustering, 0 indicates an overlap, and more positive values indicate better clustering.  \n",
        "\n",
        "Here’s the formula:\n",
        "s(i) = (b(i) - a(i))/max (a(i), b(i))\n",
        "\n",
        "a(i) = avg distance of an observation to other points in the cluster\n",
        "\n",
        "b(i) avg distance of an observation to other points in the nearest cluster.\n",
        "\n",
        "I am following the above formula and averaging it for each point for each class to get the overall silhouette score."
      ],
      "metadata": {
        "id": "WAjMbqqzHi3J"
      },
      "id": "WAjMbqqzHi3J"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "870a2f52-df9b-4c4e-8a12-a8fb73bb5041",
      "metadata": {
        "id": "870a2f52-df9b-4c4e-8a12-a8fb73bb5041"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import silhouette_score\n",
        "\n",
        "encoded_images, all_labels = encoded[:, :-1], encoded[:, -1]\n",
        "\n",
        "cnv_df = df.loc[(df.loc[:, \"Label\"] == \"CNV\") | (df.loc[:, \"Label\"] == \"NORMAL\")]\n",
        "dme_df = df.loc[(df.loc[:, \"Label\"] == \"DME\") | (df.loc[:, \"Label\"] == \"NORMAL\")]\n",
        "drusen_df = df.loc[(df.loc[:, \"Label\"] == \"DRUSEN\") | (df.loc[:, \"Label\"] == \"NORMAL\")]\n",
        "\n",
        "cnv_normal_images = cnv_df.loc[:, [\"Feature1\", \"Feature2\"]].values\n",
        "dme_normal_images = dme_df.loc[:, [\"Feature1\", \"Feature2\"]].values\n",
        "drusen_normal_images = drusen_df.loc[:, [\"Feature1\", \"Feature2\"]].values\n",
        "label = np.array([0 for _ in range(250)] + [1 for _ in range(250)])\n",
        "\n",
        "print(\"CNV-NORMAL SS = \" + str(silhouette_score(cnv_normal_images, label)))\n",
        "print(\"DME-NORMAL SS = \" + str(silhouette_score(dme_normal_images, label)))\n",
        "print(\"DRUSEN-NORMAL SS = \" + str(silhouette_score(drusen_normal_images, label)))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Finding the % of class detected as anomaly"
      ],
      "metadata": {
        "id": "YskayMNBZfqz"
      },
      "id": "YskayMNBZfqz"
    },
    {
      "cell_type": "markdown",
      "source": [
        "First, I used the describe() function to see what the mean and standard deviation for the two features plotted was (for the normal images) as I need to use those values to compute the z score later."
      ],
      "metadata": {
        "id": "uJsBwDflIP4c"
      },
      "id": "uJsBwDflIP4c"
    },
    {
      "cell_type": "code",
      "source": [
        "df.loc[df.loc[:,\"Label\"]==\"NORMAL\"].describe()"
      ],
      "metadata": {
        "id": "i4C4LHIiikEi"
      },
      "id": "i4C4LHIiikEi",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here are the values I got as per the last time I ran this notebook."
      ],
      "metadata": {
        "id": "YST4Q8UfIaYT"
      },
      "id": "YST4Q8UfIaYT"
    },
    {
      "cell_type": "code",
      "source": [
        "f1mean = 8.955534\n",
        "f1std = 5.753597\n",
        "f2mean = 4.926497\n",
        "f2std = 11.318357"
      ],
      "metadata": {
        "id": "28u8hi3FgfrY"
      },
      "id": "28u8hi3FgfrY",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "I defined the function that computes the 2D z score given a data frame."
      ],
      "metadata": {
        "id": "Z4x0sGHVI5eL"
      },
      "id": "Z4x0sGHVI5eL"
    },
    {
      "cell_type": "code",
      "source": [
        "def twoDzScore(df):\n",
        "    \"\"\"\n",
        "    Calculates the 2D z score given a data frame (using the previously found normal means and standard deviations\n",
        "    as the default normal values)\n",
        "    \"\"\"\n",
        "    z_scores = []\n",
        "    for idx, row in df.iterrows():\n",
        "      f1 = row[\"Feature1\"]\n",
        "      f2 = row[\"Feature2\"]\n",
        "      z = (((f1 - f1mean)/f1std)**2 +((f2 - f2mean)/f2std)**2)**0.5\n",
        "      z_scores.append(z)\n",
        "    df[\"z_score\"] = z_scores\n",
        "    return df"
      ],
      "metadata": {
        "id": "VlCMUhH9jrGv"
      },
      "id": "VlCMUhH9jrGv",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using the threshold of three standard deviations away from the normal mean as an anomaly, I calculated how many of each of the retinal diseased OCT images were correctly flagged as anomalies with this method."
      ],
      "metadata": {
        "id": "GSiTCWSCI-DD"
      },
      "id": "GSiTCWSCI-DD"
    },
    {
      "cell_type": "code",
      "source": [
        "cnvData = df.loc[df.loc[:,\"Label\"]==\"CNV\"].copy()\n",
        "cnvData = twoDzScore(cnvData)\n",
        "print((cnvData[\"z_score\"] > 3).sum()/len(cnvData))\n",
        "\n",
        "dmeData = df.loc[df.loc[:,\"Label\"]==\"DME\"].copy()\n",
        "dmeData = twoDzScore(dmeData)\n",
        "print((dmeData[\"z_score\"] > 3).sum()/len(dmeData))\n",
        "\n",
        "drusenData = df.loc[df.loc[:,\"Label\"]==\"DRUSEN\"].copy()\n",
        "drusenData = twoDzScore(drusenData)\n",
        "print((drusenData[\"z_score\"] > 3).sum()/len(drusenData))"
      ],
      "metadata": {
        "id": "FAmYSwsvjbsg"
      },
      "id": "FAmYSwsvjbsg",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "normalData = df.loc[df.loc[:,\"Label\"]==\"NORMAL\"].copy()\n",
        "normalData = twoDzScore(normalData)\n",
        "print((normalData[\"z_score\"] > 3).sum()/len(normalData))"
      ],
      "metadata": {
        "id": "4pmoDcPH9F5L"
      },
      "id": "4pmoDcPH9F5L",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    },
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}