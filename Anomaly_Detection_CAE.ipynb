{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Anomaly Detection with Convolutional Autoencoders using Reconstruction Loss"
      ],
      "metadata": {
        "id": "Tc5_kKl3MHVv"
      },
      "id": "Tc5_kKl3MHVv"
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is my work for constructing and testing a Convolutional Autoencoder (CAE) that helps detect Alzheimer's Disease early using images from three linked conditions. Specifically, I'm using the reconstruction loss values that my CAE has for each class to detect anomalies."
      ],
      "metadata": {
        "id": "M_r_akc4MVbf"
      },
      "id": "M_r_akc4MVbf"
    },
    {
      "cell_type": "markdown",
      "source": [
        "First, I did all relevant imports for the project and seeded everything."
      ],
      "metadata": {
        "id": "8tzLTPTZMfi6"
      },
      "id": "8tzLTPTZMfi6"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d51de17a-eeb6-47e0-b638-6c610ea9fe5c",
      "metadata": {
        "id": "d51de17a-eeb6-47e0-b638-6c610ea9fe5c"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import random\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from tqdm.notebook import tqdm\n",
        "from sklearn.manifold import TSNE\n",
        "import pandas as pd\n",
        "from PIL import Image\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, Subset\n",
        "from torchvision.datasets import ImageFolder\n",
        "from torchvision import transforms\n",
        "from torchvision.utils import make_grid\n",
        "from torch.amp import GradScaler, autocast\n",
        "\n",
        "\n",
        "def set_seed(seed):\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    random.seed(seed)\n",
        "\n",
        "set_seed(0)\n",
        "\n",
        "## Set Percent of Subset for Training the EncoderFor Classifier\n",
        "train_set_pct = 0.4"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, I mounted my google drive, where I unzipped all files downloaded in my local drive. All data was obtained from the article \"Identifying Medical Diagnoses and Treatable Diseases by Image-Based Deep Learning\" published in the journal Cell.\n",
        "\n",
        "Citation for Data Usage:\n",
        "Kermany D, Goldbaum M, Cai W et al. Identifying Medical Diagnoses and Treatable Diseases by Image-Based Deep Learning. Cell. 2018; 172(5):1122-1131. doi:10.1016/j.cell.2018.02.010."
      ],
      "metadata": {
        "id": "g-t4paVOMpYY"
      },
      "id": "g-t4paVOMpYY"
    },
    {
      "cell_type": "code",
      "source": [
        "## Mounting the Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "!rsync -ah -progress \"/content/drive/MyDrive/Colab Notebooks/Kermany Data/ZhangLabData.zip\" \"/content\"\n",
        "!unzip /content/ZhangLabData.zip\n",
        "\n",
        "PATH_TRAIN = \"/content/CellData/OCT/train\"\n",
        "PATH_TEST = \"/content/CellData/OCT/test\""
      ],
      "metadata": {
        "collapsed": true,
        "id": "D0b3H9793N61"
      },
      "id": "D0b3H9793N61",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "c73516dc-c21f-4345-b4f7-cc2d6f487df9",
      "metadata": {
        "id": "c73516dc-c21f-4345-b4f7-cc2d6f487df9"
      },
      "source": [
        "## Preparing the Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "I defined the transformations, a class for loading the images which is used later, and built the train and test set for the optical coherence tomography (OCT) images. I also split off a random subset of the train data to train my EncoderForClassifier which is defined later."
      ],
      "metadata": {
        "id": "TunRaSBnMwEO"
      },
      "id": "TunRaSBnMwEO"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "132bda24-6f5e-4007-9bce-73922a857fa4",
      "metadata": {
        "id": "132bda24-6f5e-4007-9bce-73922a857fa4"
      },
      "outputs": [],
      "source": [
        "GRAY_MEAN = [0.5]\n",
        "GRAY_STD = [0.5]\n",
        "\n",
        "train_transform = transforms.Compose(\n",
        "    [\n",
        "        transforms.Lambda(lambda x: x.convert(\"L\")),\n",
        "        transforms.Resize((128,128)),\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=GRAY_MEAN, std=GRAY_STD)\n",
        "    ]\n",
        ")\n",
        "\n",
        "val_transform = transforms.Compose(\n",
        "    [\n",
        "        transforms.Lambda(lambda x: x.convert(\"L\")),\n",
        "        transforms.Resize((128,128)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=GRAY_MEAN, std=GRAY_STD)\n",
        "    ]\n",
        ")\n",
        "\n",
        "\"\"\"\n",
        "Class for Loading Images\n",
        "\"\"\"\n",
        "class ImageLoaderDataset(nn.Module):\n",
        "    def __init__(self, path_to_folder, transform):\n",
        "      \"\"\"\n",
        "      Initializes instance of the ImageLoaderDataset Class\n",
        "\n",
        "      Parameters:\n",
        "      self (ImageLoaderDataset): instance of the class\n",
        "      path_to_folder (str): the path to the data\n",
        "      transform (transforms): the transformation to the data\n",
        "      \"\"\"\n",
        "        self.path_to_folder = path_to_folder\n",
        "        self.training_files = os.listdir(path_to_folder)\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "      \"\"\"\n",
        "      Returns the length of the data files\n",
        "\n",
        "      Parameter:\n",
        "      self (ImageLoaderDataset): instance of the class\n",
        "\n",
        "      Returns:\n",
        "      int: the length of training files\n",
        "      \"\"\"\n",
        "        return len(self.training_files)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "      \"\"\"\n",
        "      Gets the image at an index in the training files\n",
        "\n",
        "      Parameters:\n",
        "      self (ImageLoaderDataset): instance of the class\n",
        "      idx (int): the index of the image you are returning\n",
        "\n",
        "      Returns:\n",
        "      Image: the image at that index\n",
        "      \"\"\"\n",
        "        sample = self.training_files[idx]\n",
        "        path_to_sample = os.path.join(self.path_to_folder, sample)\n",
        "\n",
        "        image = Image.open(path_to_sample).convert(\"L\")\n",
        "        image = self.transform(image)\n",
        "\n",
        "        return image\n",
        "\n",
        "## Using Image Folder to Build the Train and Test Dataset ##\n",
        "train_set = ImageFolder(PATH_TRAIN, transform=train_transform)\n",
        "test_set = ImageFolder(PATH_TEST, transform=train_transform)\n",
        "\n",
        "## Assigning Each Class an index ##\n",
        "data_classes = {idx: val for (idx, val) in enumerate(train_set.classes)}\n",
        "\n",
        "## Choosing a Random Subset for Pretraining ##\n",
        "random_indices = random.sample(range(len(train_set)), int(len(train_set) * train_set_pct))\n",
        "train_set = Subset(train_set, random_indices)\n",
        "\n",
        "print(f\"Training with {len(train_set)} samples\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "62e7519b-7b02-4f08-ac63-eece6489f023",
      "metadata": {
        "id": "62e7519b-7b02-4f08-ac63-eece6489f023"
      },
      "source": [
        "## Defining ResidualBlocks & The Encoder"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, I defined a class for ResidualBlocks and the other key encoding classes: the EncoderBlocks, the Encoder, and the EncoderForClassifier, which I will later train on the subset of the data."
      ],
      "metadata": {
        "id": "OHUhAVw8N7KJ"
      },
      "id": "OHUhAVw8N7KJ"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8e879f6b-72bf-49d6-b1e0-ded9a424bd75",
      "metadata": {
        "id": "8e879f6b-72bf-49d6-b1e0-ded9a424bd75"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "ResidualBlock: a class that represents a block of residual connections\n",
        "\"\"\"\n",
        "class ResidualBlock(nn.Module):\n",
        "\n",
        "    def __init__(self,\n",
        "                 in_channels,\n",
        "                 out_channels,\n",
        "                 dropout_p=0.0):\n",
        "      \"\"\"\n",
        "      Initializes the ResidualBlock class.\n",
        "\n",
        "      Parameters:\n",
        "      self (Residual Block): instance of the class\n",
        "      in_channels (int): number of input channels\n",
        "      out_channels (int): number of output channels\n",
        "      dropout_p (float): dropout probability percent if you are randomly turning off some % of nodes, set to 0\n",
        "      \"\"\"\n",
        "        super().__init__()\n",
        "\n",
        "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=1, padding=\"same\")\n",
        "        self.norm1 = nn.BatchNorm2d(out_channels)\n",
        "\n",
        "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=\"same\")\n",
        "        self.norm2 = nn.BatchNorm2d(out_channels)\n",
        "\n",
        "        self.identity_conv = nn.Identity()\n",
        "        if in_channels != out_channels:\n",
        "            self.identity_conv = nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "      \"\"\"\n",
        "      Forward function for ResidualBlock. Applies a residual connection by applying a convolution\n",
        "      that goes from the input channels to the output channels with stride=1, applies a BatchNorm2d and\n",
        "      then the activation function. Applies a second convolution from output channels to output channels and\n",
        "      a BatchNorm2d and then activation again. Finally, it has the residual effect by adding the identity on the residual.\n",
        "\n",
        "      Parameters:\n",
        "      self (ResidualBlock): instance of the class\n",
        "      x: the input to the block\n",
        "\n",
        "      Returns:\n",
        "      x: the output after adding the residual\n",
        "      \"\"\"\n",
        "\n",
        "        residual = x\n",
        "\n",
        "        x = self.conv1(x)\n",
        "        x = self.norm1(x)\n",
        "        x = F.relu(x)\n",
        "\n",
        "        x = self.conv2(x)\n",
        "        x = self.norm2(x)\n",
        "        x = F.relu(x)\n",
        "\n",
        "        x = x + self.identity_conv(residual)\n",
        "\n",
        "        return x\n",
        "\n",
        "\"\"\"\n",
        "EncoderBlock: a class that defines the structure and ResidualBlocks that make up the Encoder\n",
        "of my CAE.\n",
        "\"\"\"\n",
        "class EncoderBlock(nn.Module):\n",
        "    def __init__(self,\n",
        "                 in_channels,\n",
        "                 out_channels,\n",
        "                 residual_blocks_per_group=1,\n",
        "                 downsample=True,\n",
        "                 dropout_p=0.0):\n",
        "      \"\"\"\n",
        "      Initializes an instance of the EncoderBlock class.\n",
        "\n",
        "      Parameters:\n",
        "      self (EncoderBlock): instance of the class\n",
        "      in_channels (int): number of input channels\n",
        "      out_channels (int): number of output channels\n",
        "      residual_blocks_per_group (int): number of ResidualBlocks per group, set to 1\n",
        "      downsample (bool): True if downsampling, else False\n",
        "      dropout_p(float): % of nodes randomly turned off, set to 0.0\n",
        "      \"\"\"\n",
        "        super().__init__()\n",
        "\n",
        "        self.encoder_block = nn.ModuleList([])\n",
        "        for i in range(residual_blocks_per_group):\n",
        "            in_c = in_channels if i == 0 else out_channels\n",
        "            self.encoder_block.append(\n",
        "                ResidualBlock(in_c, out_channels, dropout_p)\n",
        "            )\n",
        "\n",
        "        if downsample:\n",
        "            self.encoder_block.append(\n",
        "                nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=2, padding=1)\n",
        "            )\n",
        "\n",
        "    def forward(self, x):\n",
        "      \"\"\"\n",
        "      Forward function for the EncoderBlock that specifies how input flows through the network.\n",
        "\n",
        "      Parameters:\n",
        "      self (EncoderBlock): instance of the class\n",
        "      x: the input passed to the layer\n",
        "      \"\"\"\n",
        "        for block in self.encoder_block:\n",
        "            x = block(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "\"\"\"\n",
        "Encoder: encoder class for the CAE, which has a sequence of EncoderBlocks which go through a channel pattern\n",
        "and which downsample.\n",
        "\"\"\"\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self,\n",
        "                 in_channels,\n",
        "                 channel_pattern=None,\n",
        "                 residual_blocks_per_group=1,\n",
        "                 dropout_p=0.0):\n",
        "        \"\"\"\n",
        "        Initializes the Encoder class\n",
        "\n",
        "        Parameters:\n",
        "        self (Encoder): instance of the class\n",
        "        in_channels (int): number of input channels\n",
        "        channel_pattern (array): the pattern of number of channels for the encoder.\n",
        "                                If None, default is [32,64,128,256]\n",
        "        residual_blocks_per_group (int): the number of residual blocks per group, set to 1\n",
        "        dropout_p (float): percent of nodes randomly turned off, set to 0.0\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "\n",
        "        self.channel_pattern = channel_pattern\n",
        "        if channel_pattern is None:\n",
        "            self.channel_pattern = [32,64,128,256]\n",
        "\n",
        "\n",
        "        self.blocks1 = EncoderBlock(in_channels,\n",
        "                                    self.channel_pattern[0],\n",
        "                                    residual_blocks_per_group=residual_blocks_per_group,\n",
        "                                    downsample=True,\n",
        "                                    dropout_p=dropout_p)\n",
        "\n",
        "        self.blocks2 = EncoderBlock(self.channel_pattern[0],\n",
        "                                    self.channel_pattern[1],\n",
        "                                    residual_blocks_per_group=residual_blocks_per_group,\n",
        "                                    downsample=True,\n",
        "                                    dropout_p=dropout_p)\n",
        "\n",
        "        self.blocks3 = EncoderBlock(self.channel_pattern[1],\n",
        "                                    self.channel_pattern[2],\n",
        "                                    residual_blocks_per_group=residual_blocks_per_group,\n",
        "                                    downsample=True,\n",
        "                                    dropout_p=dropout_p)\n",
        "\n",
        "        self.blocks4 = EncoderBlock(self.channel_pattern[2],\n",
        "                                    self.channel_pattern[3],\n",
        "                                    residual_blocks_per_group=residual_blocks_per_group,\n",
        "                                    downsample=True,\n",
        "                                    dropout_p=dropout_p)\n",
        "\n",
        "    def forward(self, x):\n",
        "      \"\"\"\n",
        "      The forward function for the Encoder that specifies how input flows through the\n",
        "      network. It goes through all 4 encoder blocks.\n",
        "      \"\"\"\n",
        "        x = self.blocks1(x)\n",
        "        x = self.blocks2(x)\n",
        "        x = self.blocks3(x)\n",
        "        x = self.blocks4(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "\"\"\"\n",
        "EncoderForClassifier: This is the encoder which learns the classification, being\n",
        "trained later on just a percent of the data. I will use this to load pretrained weights\n",
        "to the encoder of the CAE later.\n",
        "\"\"\"\n",
        "class EncoderForClassifier(nn.Module):\n",
        "  \"\"\"\n",
        "  Initializes the EncoderForClassifier.\n",
        "\n",
        "  Parameters:\n",
        "  self (Encoder): instance of the class\n",
        "  num_classes (int): number of classes, set to four since the OCT dataset has four\n",
        "  in_channels (int): number of input channels\n",
        "  channel_pattern (array): the pattern of number of channels for the encoder.\n",
        "                           If None, default is [32,64,128,256]\n",
        "  residual_blocks_per_group (int): the number of residual blocks per group, set to 1\n",
        "  dropout_p (float): percent of nodes randomly turned off, set to 0.0\n",
        "  \"\"\"\n",
        "    def __init__(self,\n",
        "                 num_classes=4,\n",
        "                 in_channels=1,\n",
        "                 channel_pattern=None,\n",
        "                 residual_blocks_per_group=1,\n",
        "                 dropout_p=0.0):\n",
        "        super().__init__()\n",
        "\n",
        "        self.encoder = Encoder(in_channels,\n",
        "                               channel_pattern=channel_pattern,\n",
        "                               residual_blocks_per_group=residual_blocks_per_group,\n",
        "                               dropout_p=dropout_p)\n",
        "\n",
        "        self.head = nn.Linear(self.encoder.channel_pattern[-1] * 8 * 8, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Forward function for the EncoderForClassifier\n",
        "\n",
        "        Parameters:\n",
        "        self (EncoderForClassifier): instance of the class\n",
        "        x: input passed to EncoderForClassifier\n",
        "        \"\"\"\n",
        "\n",
        "        x = self.encoder(x)\n",
        "        x = x.flatten(1)\n",
        "        x = self.head(x)\n",
        "\n",
        "        return x\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "55ae5582-1b13-4560-84ed-f6d1dfbed6eb",
      "metadata": {
        "id": "55ae5582-1b13-4560-84ed-f6d1dfbed6eb"
      },
      "source": [
        "## Train Classifier on Subset"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here, I wrote my training script and trained the EncoderForClassifier on a subset of the data. I also saved the state dict of the encoder."
      ],
      "metadata": {
        "id": "-NkKIKc4N_s7"
      },
      "id": "-NkKIKc4N_s7"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "90ac2920-417b-45cd-85e1-08aedd356f84",
      "metadata": {
        "id": "90ac2920-417b-45cd-85e1-08aedd356f84"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Training function for the EncoderForClassifier\n",
        "\n",
        "Parameters:\n",
        "model: the model being trained\n",
        "device: the device being used\n",
        "epochs (int): number of epochs\n",
        "optimizer: the optimizer\n",
        "loss_fn: the loss function\n",
        "trainloader: the data loader for the training data\n",
        "valloader: the data loader for the test data\n",
        "\n",
        "Returns:\n",
        "log_training: the training log that specifies the epoch, training loss, training\n",
        "accuracy, validation loss, and validation accuracy.\n",
        "model: the model\n",
        "\"\"\"\n",
        "def train(model, device, epochs, optimizer, loss_fn, trainloader, valloader):\n",
        "    log_training = {\"epoch\": [],\n",
        "                    \"training_loss\": [],\n",
        "                    \"training_acc\": [],\n",
        "                    \"validation_loss\": [],\n",
        "                    \"validation_acc\": []}\n",
        "\n",
        "    model = model.to(device)\n",
        "    scaler = GradScaler()  # Initialize the gradient scaler for mixed precision\n",
        "\n",
        "    for epoch in range(1, epochs + 1):\n",
        "        print(f\"Starting Epoch {epoch}\")\n",
        "        training_losses, training_accuracies = [], []\n",
        "        validation_losses, validation_accuracies = [], []\n",
        "\n",
        "        model.train()  # Turn On BatchNorm and Dropout\n",
        "        for image, label in tqdm(trainloader):\n",
        "            image, label = image.to(device), label.to(device)\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # Mixed precision training block\n",
        "            with autocast(device_type=\"cuda\", dtype=torch.bfloat16):\n",
        "                out = model.forward(image)\n",
        "                ### CALCULATE LOSS ###\n",
        "                loss = loss_fn(out, label)\n",
        "\n",
        "            # Scale the loss and backpropagate\n",
        "            scaler.scale(loss).backward()\n",
        "            scaler.step(optimizer)\n",
        "            scaler.update()\n",
        "\n",
        "            training_losses.append(loss.item())\n",
        "\n",
        "            ### CALCULATE ACCURACY ###\n",
        "            predictions = torch.argmax(out, axis=1)\n",
        "            accuracy = (predictions == label).sum() / len(predictions)\n",
        "            training_accuracies.append(accuracy.item())\n",
        "\n",
        "        model.eval()  # Turn Off Batchnorm\n",
        "        for image, label in tqdm(valloader):\n",
        "            image, label = image.to(device), label.to(device)\n",
        "\n",
        "            with torch.no_grad():\n",
        "                out = model.forward(image)\n",
        "            ### CALCULATE LOSS ###\n",
        "            loss = loss_fn(out, label)\n",
        "\n",
        "            validation_losses.append(loss.item())\n",
        "\n",
        "            ### CALCULATE ACCURACY ###\n",
        "            predictions = torch.argmax(out, axis=1)\n",
        "            accuracy = (predictions == label).sum() / len(predictions)\n",
        "            validation_accuracies.append(accuracy.item())\n",
        "\n",
        "        training_loss_mean, training_acc_mean = np.mean(training_losses), np.mean(training_accuracies)\n",
        "        valid_loss_mean, valid_acc_mean = np.mean(validation_losses), np.mean(validation_accuracies)\n",
        "\n",
        "        log_training[\"epoch\"].append(epoch)\n",
        "        log_training[\"training_loss\"].append(training_loss_mean)\n",
        "        log_training[\"training_acc\"].append(training_acc_mean)\n",
        "        log_training[\"validation_loss\"].append(valid_loss_mean)\n",
        "        log_training[\"validation_acc\"].append(valid_acc_mean)\n",
        "\n",
        "        print(\"Training Loss:\", training_loss_mean)\n",
        "        print(\"Training Acc:\", training_acc_mean)\n",
        "        print(\"Validation Loss:\", valid_loss_mean)\n",
        "        print(\"Validation Acc:\", valid_acc_mean)\n",
        "\n",
        "    return log_training, model\n",
        "\n",
        "#WHY\n",
        "model = EncoderForClassifier(channel_pattern=[64,128,256,256],\n",
        "                             residual_blocks_per_group=2,\n",
        "                             dropout_p=0.2)\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "epochs = 8\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=0.0001, weight_decay=0.01)\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "trainloader = DataLoader(train_set, batch_size=64, shuffle=True, num_workers=16)\n",
        "testloader = DataLoader(test_set, batch_size=64, shuffle=False, num_workers=16)\n",
        "\n",
        "log_training, model = train(model=model,\n",
        "                            device=device,\n",
        "                            epochs=epochs,\n",
        "                            optimizer=optimizer,\n",
        "                            loss_fn=loss_fn,\n",
        "                            trainloader=trainloader,\n",
        "                            valloader=testloader)\n",
        "\n",
        "torch.save(model.encoder.state_dict(), \"deepEncoder\" + str(int(train_set_pct*100)) +\".pt\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "514e7746-2d0b-4780-848b-bb5edd992058",
      "metadata": {
        "id": "514e7746-2d0b-4780-848b-bb5edd992058"
      },
      "source": [
        "### Defining my CAE"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "I defined the main Variational Autoencoder model. Of course, I had to define relevant classes including the class for upsampling and the Decoder (along with the DecoderBlock) for this task."
      ],
      "metadata": {
        "id": "I_Fwk8v9O5_G"
      },
      "id": "I_Fwk8v9O5_G"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a0fab95b-915c-4037-9015-9c912b948d7f",
      "metadata": {
        "id": "a0fab95b-915c-4037-9015-9c912b948d7f"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "UpsampleBlock: Class for Upsampling for the CAE since Residual Connections alone don't\n",
        "change image resolution\n",
        "\"\"\"\n",
        "class UpsampleBlock(nn.Module):\n",
        "  \"\"\"\n",
        "  Initializes UpsampleBlock\n",
        "\n",
        "  Parameters:\n",
        "  self (UpsampleBlock): instance of the class\n",
        "  in_channels (int): number of input channels\n",
        "  out_channels (int): number of output channels\n",
        "  \"\"\"\n",
        "    def __init__(self,\n",
        "                 in_channels,\n",
        "                 out_channels):\n",
        "        super().__init__()\n",
        "\n",
        "        self.up = nn.Sequential(\n",
        "            nn.Upsample(scale_factor=2),\n",
        "            nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=1, padding=\"same\")\n",
        "        )\n",
        "  \"\"\"\n",
        "  Forward function for UpsampleBlock that applies upsampling to the input\n",
        "\n",
        "  Parameters:\n",
        "  self (UpSampleBlock): instance of the class\n",
        "  x: input passed into model\n",
        "  \"\"\"\n",
        "    def forward(self, x):\n",
        "        return self.up(x)\n",
        "\n",
        "\"\"\"\n",
        "DecoderBlock: a class that details the structure of residual blocks in the decoder.\n",
        "\"\"\"\n",
        "class DecoderBlock(nn.Module):\n",
        "    def __init__(self,\n",
        "                 in_channels,\n",
        "                 out_channels,\n",
        "                 residual_blocks_per_group=1,\n",
        "                 upsample=True,\n",
        "                 dropout_p=0.0):\n",
        "       \"\"\"\n",
        "      Initializes the DecoderBlock.\n",
        "\n",
        "      Parameters:\n",
        "      self (DecoderBlock): instance of the class\n",
        "      in_channels (int): number of input channels\n",
        "      out_channels (int): number of output channels\n",
        "      residual_blocks_per_group (int): number of ResidualBlocks per group, set to 1\n",
        "      upsample (bool): True if upsampling, else False\n",
        "      dropout_p(float): % of nodes randomly turned off, set to 0.0\n",
        "\n",
        "      \"\"\"\n",
        "        super().__init__()\n",
        "\n",
        "        self.encoder_block = nn.ModuleList([])\n",
        "        for i in range(residual_blocks_per_group):\n",
        "            in_c = in_channels if i == 0 else out_channels\n",
        "            self.encoder_block.append(\n",
        "                ResidualBlock(in_c, out_channels, dropout_p)\n",
        "            )\n",
        "\n",
        "        if upsample:\n",
        "            self.encoder_block.append(\n",
        "                UpsampleBlock(out_channels, out_channels)\n",
        "            )\n",
        "  \"\"\"\n",
        "  Forward function for the DecoderBlock that specifies how input\n",
        "  flows through the network's layers.\n",
        "\n",
        "  Parameters:\n",
        "  self (DecoderBlock): instance of the class\n",
        "  x: the input passed to the layer\n",
        "  \"\"\"\n",
        "    def forward(self, x):\n",
        "\n",
        "        for block in self.encoder_block:\n",
        "            x = block(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "\"\"\"\n",
        "Decoder: decoder class for the CAE, which has a sequence of DecoderBlocks which go through a channel pattern\n",
        "and which upsample.\n",
        "\"\"\"\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self,\n",
        "                 out_channels,\n",
        "                 channel_pattern=None,\n",
        "                 residual_blocks_per_group=1,\n",
        "                 dropout_p=0.0):\n",
        "      \"\"\"\n",
        "      Initializes the Encoder class\n",
        "\n",
        "      Parameters:\n",
        "      self (Decoder): instance of the class\n",
        "      out_channels (int): number of output channels\n",
        "      channel_pattern (array): the pattern of number of channels for the encoder.\n",
        "                                If None, default is [32,64,128,256]\n",
        "      residual_blocks_per_group (int): the number of residual blocks per group, set to 1\n",
        "      dropout_p (float): percent of nodes randomly turned off, set to 0.0\n",
        "      \"\"\"\n",
        "        super().__init__()\n",
        "\n",
        "        self.channel_pattern = channel_pattern\n",
        "        if channel_pattern is None:\n",
        "            self.channel_pattern = [32,64,128,256]\n",
        "        self.channel_pattern = list(reversed(self.channel_pattern))\n",
        "\n",
        "        self.blocks1 = DecoderBlock(self.channel_pattern[0],\n",
        "                                    self.channel_pattern[1],\n",
        "                                    residual_blocks_per_group=residual_blocks_per_group,\n",
        "                                    upsample=True,\n",
        "                                    dropout_p=dropout_p)\n",
        "\n",
        "        self.blocks2 = DecoderBlock(self.channel_pattern[1],\n",
        "                                    self.channel_pattern[2],\n",
        "                                    residual_blocks_per_group=residual_blocks_per_group,\n",
        "                                    upsample=True,\n",
        "                                    dropout_p=dropout_p)\n",
        "\n",
        "        self.blocks3 = DecoderBlock(self.channel_pattern[2],\n",
        "                                    self.channel_pattern[3],\n",
        "                                    residual_blocks_per_group=residual_blocks_per_group,\n",
        "                                    upsample=True,\n",
        "                                    dropout_p=dropout_p)\n",
        "\n",
        "        self.blocks4 = DecoderBlock(self.channel_pattern[3],\n",
        "                                    out_channels,\n",
        "                                    residual_blocks_per_group=residual_blocks_per_group,\n",
        "                                    upsample=True,\n",
        "                                    dropout_p=dropout_p)\n",
        "  \"\"\"\n",
        "  The forward function for the Decoder that specifies how input flows through the\n",
        "  network. It goes through all 4 decoder blocks.\n",
        "  \"\"\"\n",
        "    def forward(self, x):\n",
        "\n",
        "        x = self.blocks1(x)\n",
        "        x = self.blocks2(x)\n",
        "        x = self.blocks3(x)\n",
        "        x = self.blocks4(x)\n",
        "\n",
        "        return x\n",
        "\"\"\"\n",
        "CAE: Convolutional Autoencoder\n",
        "\"\"\"\n",
        "class CAE(nn.Module):\n",
        "    def __init__(self,\n",
        "                 in_channels,\n",
        "                 channel_pattern=None,\n",
        "                 residual_blocks_per_group=1,\n",
        "                 dropout_p=0.0):\n",
        "      \"\"\"\n",
        "      Initializes the CAE class\n",
        "\n",
        "      Parameters:\n",
        "      self (CAE): instance of the class\n",
        "      in_channels (int): number of input channels\n",
        "      latent_channels (int): number of latent channels\n",
        "      channel_pattern (array): the pattern of number of channels for the encoder.\n",
        "                                If None, default is [32,64,128,256]\n",
        "      residual_blocks_per_group (int): the number of residual blocks per group, set to 1\n",
        "      dropout_p (float): percent of nodes randomly turned off, set to 0.0\n",
        "      \"\"\"\n",
        "        super().__init__()\n",
        "\n",
        "        self.channel_pattern = channel_pattern\n",
        "        if channel_pattern is None:\n",
        "            self.channel_pattern = [32,64,128,256]\n",
        "\n",
        "        self.encoder = Encoder(in_channels=in_channels,\n",
        "                               channel_pattern=self.channel_pattern,\n",
        "                               residual_blocks_per_group=residual_blocks_per_group,\n",
        "                               dropout_p=dropout_p)\n",
        "\n",
        "        self.decoder = Decoder(out_channels=in_channels,\n",
        "                               channel_pattern=self.channel_pattern,\n",
        "                               residual_blocks_per_group=residual_blocks_per_group,\n",
        "                               dropout_p=dropout_p)\n",
        "\n",
        "  \"\"\"\n",
        "  The forward function for the CAE. Goes through the encoder and decoder.\n",
        "\n",
        "  Parameters:\n",
        "  self (CAE): instance of class\n",
        "  x: input\n",
        "\n",
        "  Returns:\n",
        "  enc: the encoded features\n",
        "  dec: the decoded image\n",
        "  \"\"\"\n",
        "    def forward(self, x):\n",
        "\n",
        "        with torch.no_grad():\n",
        "            enc = self.encoder(x)\n",
        "\n",
        "        dec = self.decoder(enc)\n",
        "        dec = F.tanh(dec)\n",
        "\n",
        "        return enc, dec"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9b9afff4-baf8-4b35-a68e-f469fb55143d",
      "metadata": {
        "id": "9b9afff4-baf8-4b35-a68e-f469fb55143d"
      },
      "source": [
        "### Training the CAE ###"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, I trained the CAE."
      ],
      "metadata": {
        "id": "677FqocYPyyQ"
      },
      "id": "677FqocYPyyQ"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e59045c4-574f-4906-9773-8d2878b23fc4",
      "metadata": {
        "id": "e59045c4-574f-4906-9773-8d2878b23fc4",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "def train(model,\n",
        "          train_set,\n",
        "          test_set,\n",
        "          batch_size,\n",
        "          training_iterations,\n",
        "          evaluation_iterations,\n",
        "          verbose=False):\n",
        "  \"\"\"\n",
        "  Training Script for the CAE.\n",
        "\n",
        "  Parameters:\n",
        "  model: the model\n",
        "  kl_weight: the weight for the kL divergence loss\n",
        "  train_set: the train set\n",
        "  test_set: the test set\n",
        "  batch_size: the batch size\n",
        "  training_iterations: the number of training iterations\n",
        "  evaluation_iterations: the number of evaluation iterations\n",
        "\n",
        "  Returns:\n",
        "  model: the model\n",
        "  train_losses: the train losses\n",
        "  evaluation_losses: the evaluation losses\n",
        "  encoded_data_per_eval: the encoded data\n",
        "  \"\"\"\n",
        "    print(\"Training Model!\")\n",
        "    print(model)\n",
        "\n",
        "    ## Set Device, the Loss Function, the DataLoaders, and the Optimizer ##\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    model = model.to(device)\n",
        "    huber_loss = torch.nn.HuberLoss(reduction='mean', delta=1.0)\n",
        "    trainloader = DataLoader(train_set, batch_size=batch_size, shuffle=True, num_workers=8)\n",
        "    testloader = DataLoader(test_set, batch_size=batch_size, shuffle=True, num_workers=8)\n",
        "    optimizer = optim.Adam(model.parameters(), lr=0.0005)\n",
        "\n",
        "    train_loss = []\n",
        "    evaluation_loss = []\n",
        "    train_losses = []\n",
        "    evaluation_losses = []\n",
        "    encoded_data_per_eval = []\n",
        "\n",
        "    ## Progress Bar ##\n",
        "    pbar = tqdm(range(training_iterations))\n",
        "\n",
        "    train = True\n",
        "    step_counter = 0\n",
        "    while train:\n",
        "\n",
        "        for images in trainloader:\n",
        "\n",
        "            images = images.to(device)\n",
        "            encoded, reconstruction = model(images)\n",
        "            loss = huber_loss(reconstruction, images)\n",
        "            train_loss.append(loss.item())\n",
        "\n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "\n",
        "            optimizer.step()\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            if step_counter % evaluation_iterations == 0:\n",
        "\n",
        "                model.eval()\n",
        "                encoded_evaluations = []\n",
        "\n",
        "                for images in testloader:\n",
        "\n",
        "                    images = images.to(device)\n",
        "                    encoded, reconstruction = model(images)\n",
        "                    loss = huber_loss(reconstruction, images)\n",
        "                    evaluation_loss.append(loss.item())\n",
        "\n",
        "\n",
        "                train_loss = np.mean(train_loss)\n",
        "                evaluation_loss = np.mean(evaluation_loss)\n",
        "\n",
        "                train_losses.append(train_loss)\n",
        "                evaluation_losses.append(evaluation_loss)\n",
        "\n",
        "                print(\"Training Loss\", train_loss)\n",
        "                print(\"Evaluation Loss\", evaluation_loss)\n",
        "\n",
        "                train_loss = []\n",
        "                evaluation_loss = []\n",
        "                model.train()\n",
        "\n",
        "                images = images[:4].cpu()\n",
        "                gens = reconstruction[:4].cpu()\n",
        "\n",
        "                images = (images + 1) / 2\n",
        "                gens = torch.clamp(((gens + 1) / 2), 0, 1)\n",
        "\n",
        "                combined = torch.cat([images, gens], dim=0)\n",
        "\n",
        "                grid = make_grid(combined, nrow=4)\n",
        "\n",
        "                plt.imshow(grid.permute(1, 2, 0))\n",
        "                plt.axis('off')\n",
        "                plt.show()\n",
        "\n",
        "            step_counter += 1\n",
        "            pbar.update(1)\n",
        "\n",
        "            if step_counter >= training_iterations:\n",
        "                print(\"Completed Training.\")\n",
        "                train = False\n",
        "                break\n",
        "\n",
        "    ## Storing Encoded Data ##\n",
        "    encoded_data_per_eval = [np.array(i) for i in encoded_data_per_eval]\n",
        "\n",
        "    print(\"Final Training Loss\", train_losses[-1])\n",
        "    print(\"Final Evaluation Loss\", evaluation_losses[-1])\n",
        "\n",
        "    return model, train_losses, evaluation_losses, encoded_data_per_eval\n",
        "\n",
        "\n",
        "conv_model = CAE(1, channel_pattern=[64, 128, 256, 256], residual_blocks_per_group=2)\n",
        "\n",
        "conv_model.encoder.load_state_dict(torch.load(\"deepEncoder\" + str(int(train_set_pct*100)) +\".pt\", weights_only=True))\n",
        "\n",
        "train_set = ImageLoaderDataset(\"CellData/OCT/train/NORMAL\", transform=train_transform)\n",
        "test_set = ImageLoaderDataset(\"CellData/OCT/test/NORMAL\", transform=val_transform)\n",
        "\n",
        "conv_model, train_losses, evaluation_losses, conv_encoded_data_per_eval = train(conv_model,\n",
        "                                                                                train_set,\n",
        "                                                                                test_set,\n",
        "                                                                                batch_size=32,\n",
        "                                                                                training_iterations=15000,\n",
        "                                                                                evaluation_iterations=2500)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "I saved the CAE's state dictionary."
      ],
      "metadata": {
        "id": "K8cNNLgeQs8d"
      },
      "id": "K8cNNLgeQs8d"
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(model.encoder.state_dict(), \"cae18_\"+ str(train_set_pct*100)+\"_lD.pt\")"
      ],
      "metadata": {
        "id": "2qMjFiWE4x1t"
      },
      "id": "2qMjFiWE4x1t",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Building Boxplots"
      ],
      "metadata": {
        "id": "rGxJ6auTQyZ5"
      },
      "id": "rGxJ6auTQyZ5"
    },
    {
      "cell_type": "markdown",
      "source": [
        "I wanted to build boxplots to conpare the range, mean, and other key features about the reconstruction loss by class for my CAE.\n",
        "\n",
        "I evaluated the model and saved the losses for each retinal disease to lists. I created a data frame using a dictionary that linked each class label to that class's losses.\n",
        "\n",
        "I saved the losses to a txt file as well, in case I want to use it later. But, the main thing I did was using seaborn and created plots for the normal reconstruction losses in comparison with each retinal condition.\n",
        "\n",
        "I also created a plot with all the three conditions and the normal losses' to compare them."
      ],
      "metadata": {
        "id": "uxbOW7VTRMRK"
      },
      "id": "uxbOW7VTRMRK"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c00085ab-f18e-4472-be7d-1699090b64b7",
      "metadata": {
        "id": "c00085ab-f18e-4472-be7d-1699090b64b7"
      },
      "outputs": [],
      "source": [
        "normal_set = ImageLoaderDataset(\"/content/CellData/OCT/test/NORMAL\", transform=val_transform)\n",
        "dme_set = ImageLoaderDataset(\"/content/CellData/OCT/test/DME\", transform=val_transform)\n",
        "drusen_set = ImageLoaderDataset(\"/content/CellData/OCT/test/DRUSEN\", transform=val_transform)\n",
        "cnv_set = ImageLoaderDataset(\"/content/CellData/OCT/test/CNV\", transform=val_transform)\n",
        "\n",
        "def eval(model, dataset):\n",
        "\n",
        "    model.eval()\n",
        "    model = model.to(\"cuda\")\n",
        "\n",
        "    huber_loss = torch.nn.L1Loss(reduction='none')\n",
        "\n",
        "    loader = DataLoader(dataset, batch_size=32)\n",
        "\n",
        "    losses = []\n",
        "    for images in loader:\n",
        "\n",
        "        images = images.to(\"cuda\")\n",
        "        with torch.no_grad():\n",
        "            enc, dec = model(images)\n",
        "\n",
        "        loss = huber_loss(dec, images)\n",
        "        loss = loss.mean(dim=[1,2,3])\n",
        "\n",
        "        losses.extend(loss.cpu().tolist())\n",
        "\n",
        "    return losses\n",
        "\n",
        "normal_losses = eval(conv_model, normal_set)\n",
        "cnv_losses = eval(conv_model, cnv_set)\n",
        "drusen_losses = eval(conv_model, drusen_set)\n",
        "dme_losses = eval(conv_model, dme_set)\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "## Initialize Data to Dicts ##\n",
        "d = {'CNV': cnv_losses,\n",
        "     'DME': dme_losses,\n",
        "     'DRUSEN': drusen_losses,\n",
        "     'Normal': normal_losses\n",
        "     }\n",
        "## Creates Dataframe. ##\n",
        "df = pd.DataFrame(d)\n",
        "\n",
        "print(df)\n",
        "df.to_csv('losses by class'+str(int(train_set_pct*100))+'.txt', sep='\\t', index=False)\n",
        "\n",
        "cn = {'CNV': cnv_losses,\n",
        "     'Normal': normal_losses\n",
        "     }\n",
        "cnn = {\"CNV\": \"red\", \"Normal\":\"purple\"}\n",
        "sns.boxplot(data = cn, width=0.3,palette = cnn)\n",
        "plt.xlabel(\"Class\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.title(\"CNV and Normal Loss for CAE trained on \" +str(int(train_set_pct*100)) +\"% of data\")\n",
        "plt.savefig(\"CNV-NORMAL_LossDetection\"+ str(int(train_set_pct*100))+\".jpeg\")\n",
        "plt.show()\n",
        "\n",
        "dn = {'DRUSEN': drusen_losses,\n",
        "     'Normal': normal_losses\n",
        "     }\n",
        "dn_palette = {'DRUSEN': 'green', 'Normal': 'purple'}\n",
        "sns.boxplot(data = dn, width=0.3, palette = dn_palette)\n",
        "plt.xlabel(\"Class\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.title(\"DRUSEN and Normal Loss for CAE trained on \" +str(int(train_set_pct*100)) +\"% of data\")\n",
        "plt.savefig(\"DRUSEN-NORMAL_LossDetection\" + str(int(train_set_pct*100))+\".jpeg\")\n",
        "plt.show()\n",
        "\n",
        "\n",
        "dmn = {'DME': dme_losses,\n",
        "     'Normal': normal_losses\n",
        "     }\n",
        "dmn_palette = {'DME': 'blue', 'Normal': 'purple'}\n",
        "sns.boxplot(data = dmn, width=0.3, palette = dmn_palette)\n",
        "plt.xlabel(\"Class\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.title(\"DME and Normal Loss for CAE trained on \" +str(int(train_set_pct*100)) +\"% of data\")\n",
        "plt.savefig(\"DME-NORMAL_LossDetection\" +str(int(train_set_pct*100)) + \".jpeg\")\n",
        "plt.show()\n",
        "\n",
        "sns.boxplot(data = df, palette = \"Set1\")\n",
        "plt.xlabel(\"Class\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.title(\"Loss Detection for CAE trained on \" +str(int(train_set_pct*100)) +\"% of data\")\n",
        "plt.savefig(\"LossDetection\" +str(int(train_set_pct*100)) + \".jpeg\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Detecting Anomalies"
      ],
      "metadata": {
        "id": "A7ci8QUQSEyq"
      },
      "id": "A7ci8QUQSEyq"
    },
    {
      "cell_type": "markdown",
      "source": [
        "I used the threshold of three standard deviations away from the normal mean to flag an anomaly.\n",
        "\n",
        "So, I used the means and standard deviations of each class (with respect to that of the normal losses' mean and standard deviation) and calculated the z score for each class. Then I checked if it was > 3, flagging those loss values which were more than three standard deviations from the mean as anomalies.\n",
        "\n",
        "I also saved this data (as to how much of each class was marked as an anomaly) to a text file."
      ],
      "metadata": {
        "id": "Drf8s81QSsYm"
      },
      "id": "Drf8s81QSsYm"
    },
    {
      "cell_type": "code",
      "source": [
        "normal_mean = np.mean(normal_losses)\n",
        "n_std = np.std(normal_losses)\n",
        "\n",
        "anomaly_count_cnv = 0\n",
        "for i in range(len(cnv_losses)):\n",
        "  z = cnv_losses[i]\n",
        "  z_score = (z - normal_mean) / n_std\n",
        "  if z_score >= 3:\n",
        "    anomaly_count_cnv = anomaly_count_cnv + 1\n",
        "print(\"The % we can detect as anomaly for CNV images is \" + str(anomaly_count_cnv/len(cnv_losses)))\n",
        "\n",
        "\n",
        "anomaly_count_dme = 0\n",
        "for i in range(len(dme_losses)):\n",
        "  z = dme_losses[i]\n",
        "  z_score = (z - normal_mean) / n_std\n",
        "  if z_score >= 3:\n",
        "    anomaly_count_dme = anomaly_count_dme + 1\n",
        "print(\"The % we can detect as anomaly for DME images is \" + str(anomaly_count_dme/len(dme_losses)))\n",
        "\n",
        "anomaly_count_drusen = 0\n",
        "for i in range(len(drusen_losses)):\n",
        "  z = drusen_losses[i]\n",
        "  z_score = (z - normal_mean) / n_std\n",
        "  if z_score >= 3:\n",
        "    anomaly_count_drusen = anomaly_count_drusen + 1\n",
        "print(\"The % we can detect as anomaly for DRUSEN images is \" + str(anomaly_count_drusen/len(drusen_losses)))\n",
        "\n",
        "anomaly = [\"The % we can detect as anomaly for CNV images is \" + str(anomaly_count_cnv/len(cnv_losses)),\n",
        "           \"The % we can detect as anomaly for DME images is \" + str(anomaly_count_dme/len(dme_losses)),\n",
        "           \"The % we can detect as anomaly for DRUSEN images is \" + str(anomaly_count_drusen/len(drusen_losses))]\n",
        "with open(\"anomaly_report_100.txt\", \"w\") as file:\n",
        "    print(anomaly, file=file)"
      ],
      "metadata": {
        "id": "2dCViBFE-V1m"
      },
      "id": "2dCViBFE-V1m",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    },
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}